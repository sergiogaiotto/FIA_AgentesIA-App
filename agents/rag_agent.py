# agents/rag_agent.py - Comentado Linha a Linha

# Agente RAG - Retrieval-Augmented Generation com Pinecone
# Coment√°rio descritivo: agente especializado em pesquisa sem√¢ntica com RAG

import os
# M√≥dulo padr√£o para intera√ß√£o com sistema operacional

import json
# M√≥dulo para manipula√ß√£o de dados JSON

from typing import List, Dict, Any, Optional
# Type hints para melhor documenta√ß√£o e type safety

import asyncio
# Biblioteca para programa√ß√£o ass√≠ncrona

# Imports para embeddings e LLM
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# ChatOpenAI: modelo de chat da OpenAI
# OpenAIEmbeddings: gera√ß√£o de embeddings usando OpenAI

from langchain_core.messages import HumanMessage, SystemMessage
# Tipos de mensagem padronizados do LangChain

# Imports para RAG
from langchain.text_splitter import RecursiveCharacterTextSplitter
# Text splitter para dividir documentos em chunks
# Recursive: tenta manter contexto sem√¢ntico

from langchain_community.vectorstores import Pinecone as LangChainPinecone
# Integra√ß√£o LangChain com Pinecone
# Abstra√ß√£o de alto n√≠vel para vector database

# Pinecone SDK
from pinecone import Pinecone, ServerlessSpec
# SDK oficial do Pinecone
# ServerlessSpec: configura√ß√£o para mode serverless

# Pydantic para modelos de dados
from pydantic import BaseModel, Field
# BaseModel: valida√ß√£o e serializa√ß√£o
# Field: metadados para campos

# Carregamento de vari√°veis
from dotenv import load_dotenv
# Carregamento de configura√ß√£o de arquivo .env

# Firecrawl para coleta de dados
from firecrawl import FirecrawlApp
# SDK para web scraping estruturado

# Carrega vari√°veis de ambiente
load_dotenv()


# ===============================
# MODELOS PYDANTIC
# ===============================

class RAGQuery(BaseModel):
    """Modelo para queries RAG"""
    query: str = Field(..., description="Consulta do usu√°rio")
    # Campo obrigat√≥rio: pergunta do usu√°rio
    # Field(...): campo obrigat√≥rio com metadados
    
    top_k: int = Field(default=4, description="N√∫mero de documentos similares")
    # Quantidade de documentos para retrieval
    # Default 5: balanceia relev√¢ncia e velocidade
    
    threshold: float = Field(default=0.1, description="Threshold de similaridade")
    # Limite m√≠nimo de similaridade sem√¢ntica
    # 0.1: valor pouco conservador que filtra resultados irrelevantes (para podermos testar)


class RAGDocument(BaseModel):
    """Modelo para documentos indexados"""
    id: str = Field(..., description="ID √∫nico do documento")
    # Identificador √∫nico no Pinecone
    
    content: str = Field(..., description="Conte√∫do do documento")
    # Texto completo do documento
    
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Metadados")
    # Informa√ß√µes adicionais: URL, t√≠tulo, data, etc.
    # default_factory=dict: cria dict vazio por padr√£o
    
    score: Optional[float] = Field(default=None, description="Score de similaridade")
    # Score de similaridade sem√¢ntica (0-1)
    # None por padr√£o, preenchido durante busca


class RAGResponse(BaseModel):
    """Modelo para resposta RAG"""
    answer: str = Field(..., description="Resposta gerada")
    # Resposta final do LLM com contexto
    
    sources: List[RAGDocument] = Field(default_factory=list, description="Documentos fonte")
    # Documentos usados para gerar resposta
    # Transpar√™ncia: usu√°rio v√™ fontes
    
    query: str = Field(..., description="Query original")
    # Query que gerou a resposta
    
    confidence: Optional[float] = Field(default=None, description="Confian√ßa da resposta")
    # N√≠vel de confian√ßa baseado nos scores


# ===============================
# SERVI√áO PINECONE
# ===============================

class PineconeService:
    """Servi√ßo para integra√ß√£o com Pinecone"""
    # Service class: encapsula opera√ß√µes do Pinecone
    # Abstra√ß√£o: isola complexidade da API Pinecone
    
    def __init__(self, index_name: str = "fia-agente-ia"):
        """Inicializa servi√ßo Pinecone"""
        # index_name: nome do √≠ndice no Pinecone
        # Default: nome descritivo para base de conhecimento
        
        # Valida√ß√£o de API key
        self.api_key = os.getenv("PINECONE_API_KEY")
        if not self.api_key:
            raise ValueError("PINECONE_API_KEY n√£o encontrada nas vari√°veis de ambiente")
        # Fail-fast: falha imediatamente se n√£o configurado
        
        # Inicializa cliente Pinecone
        self.pc = Pinecone(api_key=self.api_key)
        # Cliente principal para opera√ß√µes Pinecone
        
        self.index_name = index_name
        # Armazena nome do √≠ndice
        
        # Configura√ß√£o de embeddings
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            # Modelo otimizado: balanceia qualidade e custo
            # text-embedding-3-small: vers√£o eficiente da OpenAI
            openai_api_key=os.getenv("OPENAI_API_KEY")
            # Chave da OpenAI para embeddings
        )
        
        # Dimens√£o dos embeddings
        self.dimension = 1536
        # text-embedding-3-small produz vetores de 1536 dimens√µes
        # Fixo: dimens√£o n√£o pode mudar ap√≥s cria√ß√£o do √≠ndice
        
        # Inicializa √≠ndice
        self.index = None
        # Ser√° inicializado em setup_index()
        
    async def setup_index(self):
        """Configura √≠ndice Pinecone (cria se n√£o existir)"""
        # M√©todo async para setup inicial
        # Verifica/cria √≠ndice conforme necess√°rio
        
        try:
            # Lista √≠ndices existentes
            existing_indexes = [idx.name for idx in self.pc.list_indexes()]
            # list_indexes(): retorna todos os √≠ndices da conta
            # Extrai apenas nomes para verifica√ß√£o
            
            if self.index_name not in existing_indexes:
                # Cria √≠ndice se n√£o existir
                print(f"Criando √≠ndice Pinecone: {self.index_name}")
                
                self.pc.create_index(
                    name=self.index_name,
                    # Nome do √≠ndice
                    dimension=self.dimension,
                    # Dimens√£o dos vetores (deve ser consistente)
                    metric="cosine",
                    # M√©trica de similaridade: cosine similarity
                    # Boa para embeddings de texto
                    spec=ServerlessSpec(
                        cloud="aws",
                        region="us-east-1"
                    )
                    # Configura√ß√£o serverless: custo baseado em uso
                    # AWS us-east-1: regi√£o com melhor performance
                )
                
                # Aguarda cria√ß√£o do √≠ndice
                print("Aguardando cria√ß√£o do √≠ndice...")
                await asyncio.sleep(10)
                # Sleep: √≠ndice precisa de tempo para ficar ready
            
            # Conecta ao √≠ndice
            self.index = self.pc.Index(self.index_name)
            # Objeto Index para opera√ß√µes CRUD
            print(f"‚úÖ Conectado ao √≠ndice: {self.index_name}")
            
        except Exception as e:
            print(f"‚ùå Erro ao configurar Pinecone: {e}")
            raise
            # Re-raise: erro cr√≠tico que impede funcionamento
    
    async def add_documents(self, documents: List[RAGDocument]) -> bool:
        """Adiciona documentos ao √≠ndice"""
        # Batch insert de documentos
        # Retorna bool: sucesso/falha da opera√ß√£o
        
        if not self.index:
            await self.setup_index()
        # Lazy initialization: configura √≠ndice se necess√°rio
        
        try:
            # Prepara vetores para inser√ß√£o
            vectors = []
            # Lista de tuplas (id, embedding, metadata)
            
            for doc in documents:
                # Gera embedding do conte√∫do
                embedding = await self._generate_embedding(doc.content)
                # Embedding: representa√ß√£o vetorial do texto
                
                # Prepara vetor para Pinecone
                vector = {
                    "id": doc.id,
                    # ID √∫nico: chave prim√°ria no Pinecone
                    "values": embedding,
                    # values: vetor de embeddings
                    "metadata": {
                        **doc.metadata,
                        # Spread existing metadata
                        "content": doc.content[:512],
                        # Texto truncado para metadados
                        # Pinecone tem limite de tamanho para metadata
                    }
                }
                vectors.append(vector)
            
            # Insere em batch
            self.index.upsert(vectors=vectors)
            # upsert: insert ou update se ID j√° existir
            # Batch operation: mais eficiente que inser√ß√µes individuais
            
            print(f"‚úÖ {len(documents)} documentos adicionados ao Pinecone")
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao adicionar documentos: {e}")
            return False
            # Graceful degradation: retorna False em vez de falhar
    
    async def search(self, query: str, top_k: int = 4, threshold: float = 0.1) -> List[RAGDocument]:
        """Busca documentos similares √† query"""
        # Semantic search: busca por similaridade sem√¢ntica
        # top_k: quantidade de resultados
        # threshold: filtro de qualidade
        
        if not self.index:
            await self.setup_index()
        # Garante que √≠ndice est√° configurado
        
        try:
            # Gera embedding da query
            query_embedding = await self._generate_embedding(query)
            # Mesmo modelo usado para indexa√ß√£o
            # Consist√™ncia: embeddings compar√°veis
            
            # Busca no Pinecone
            results = self.index.query(
                vector=query_embedding,
                # Vetor da query para compara√ß√£o
                top_k=top_k,
                # Quantidade m√°xima de resultados
                include_metadata=True,
                # Inclui metadados na resposta
                include_values=False
                # N√£o inclui embeddings (economiza banda)
            )
            
            # Processa resultados
            documents = []
            for match in results.matches:
                # Filtra por threshold
                if match.score >= threshold:
                    # score: similaridade cosine (0-1)
                    
                    doc = RAGDocument(
                        id=match.id,
                        content=match.metadata.get("content", ""),
                        # Recupera conte√∫do dos metadados
                        metadata=match.metadata,
                        score=match.score,
                        # Inclui score para ranking
                        threshold=threshold
                        # Inclui threshold para lembrar do valor definido
                    )
                    documents.append(doc)
            
            print(f"üîç Encontrados {len(documents)} documentos relevantes")
            return documents
            
        except Exception as e:
            print(f"‚ùå Erro na busca: {e}")
            return []
            # Lista vazia em caso de erro
    
    async def _generate_embedding(self, text: str) -> List[float]:
        """Gera embedding para texto"""
        # M√©todo privado: uso interno
        # Async: opera√ß√£o pode ser lenta
        
        try:
            # Usa OpenAI Embeddings
            embedding = self.embeddings.embed_query(text)
            # embed_query: otimizado para queries (vs documentos)
            return embedding
            
        except Exception as e:
            print(f"‚ùå Erro ao gerar embedding: {e}")
            # Fallback: vetor zero
            return [0.0] * self.dimension
            # Lista de zeros com dimens√£o correta
    
    async def get_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas do √≠ndice"""
        # M√©todo de diagn√≥stico
        # √ötil para monitoramento e debugging
        
        if not self.index:
            return {"status": "not_initialized"}
        
        try:
            stats = self.index.describe_index_stats()
            # describe_index_stats(): metadados do √≠ndice
            
            return {
                "status": "active",
                "total_vectors": stats.total_vector_count,
                # Quantidade total de vetores indexados
                "dimension": stats.dimension,
                # Dimens√£o dos vetores
                "index_fullness": stats.index_fullness,
                # Percentual de utiliza√ß√£o do √≠ndice
                "namespaces": len(stats.namespaces) if stats.namespaces else 0
                # Quantidade de namespaces (organiza√ß√£o l√≥gica)
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }


# ===============================
# AGENTE RAG PRINCIPAL
# ===============================

class RAGAgent:
    """Agente especializado em Retrieval-Augmented Generation"""
    # Classe principal que orquestra RAG workflow
    # Combina: retrieval (Pinecone) + generation (OpenAI)
    
    def __init__(self, index_name: str = "fia-agente-ia"):
        """Inicializa agente RAG"""
        
        # Valida√ß√£o de chaves de API
        required_keys = ["PINECONE_API_KEY", "OPENAI_API_KEY"]
        for key in required_keys:
            if not os.getenv(key):
                raise ValueError(f"{key} n√£o encontrada nas vari√°veis de ambiente")
        # Valida√ß√£o em loop: todas as chaves necess√°rias
        
        # Inicializa servi√ßos
        self.pinecone_service = PineconeService(index_name)
        # Servi√ßo de vector database
        
        self.llm = ChatOpenAI(
            model="gpt-4.1-mini",
            # Modelo otimizado: boa qualidade, custo menor
            temperature=0.0,
            # Baixa temperatura: respostas mais determin√≠sticas
            # RAG precisa de consist√™ncia e precis√£o
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # Configura√ß√£o de text splitting
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=512,
            # Tamanho do chunk: balance entre contexto e performance
            # 512 chars: suficiente para par√°grafos completos
            chunk_overlap=128,
            # Overlap: mant√©m continuidade sem√¢ntica
            # 128 chars: overlap significativo mas n√£o excessivo
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
            # Ordem de prioridade para quebras
            # Prioriza quebras sem√¢nticas (par√°grafos, frases)
        )
        
        # Firecrawl para coleta de dados
        firecrawl_key = os.getenv("FIRECRAWL_API_KEY")
        self.firecrawl = FirecrawlApp(api_key=firecrawl_key) if firecrawl_key else None
        # Opcional: permite funcionar sem Firecrawl
        
        # Sistema de prompt
        self.system_prompt = """Voc√™ √© um assistente especializado em RAG (Retrieval-Augmented Generation).

                            Sua fun√ß√£o √©:
                            1. Analisar a pergunta do usu√°rio
                            2. Usar os documentos fornecidos como contexto
                            3. Gerar respostas precisas e bem fundamentadas
                            4. Citar as fontes utilizadas
                            
                            Diretrizes:
                            - Base suas respostas APENAS nos documentos fornecidos
                            - Se n√£o tiver informa√ß√£o suficiente, seja honesto sobre isso
                            - Cite espec√≠ficamente as fontes relevantes
                            - Mantenha respostas claras e objetivas
                            - Use formata√ß√£o markdown quando apropriado
                            
                            Sempre inclua uma se√ß√£o "Fontes:" no final da resposta."""
                            # System prompt espec√≠fico para RAG
                            # Enfatiza: precis√£o, cita√ß√£o de fontes, honestidade
    
    async def initialize(self):
        """Inicializa agente (configura Pinecone)"""
        # M√©todo de inicializa√ß√£o async
        # Separado do __init__ para opera√ß√µes async
        
        await self.pinecone_service.setup_index()
        print("‚úÖ RAG Agent inicializado")
    
    async def add_knowledge_from_url(self, url: str) -> bool:
        """Adiciona conhecimento a partir de URL"""
        # M√©todo para expandir base de conhecimento
        # URL ‚Üí scraping ‚Üí chunking ‚Üí embedding ‚Üí indexing
        
        if not self.firecrawl:
            print("‚ùå Firecrawl n√£o configurado")
            return False
        
        try:
            print(f"üåê Fazendo scraping de: {url}")
            
            # Scraping da URL
            scraped = self.firecrawl.scrape_url(url, formats=["markdown"])
            # Markdown: formato estruturado, f√°cil de processar
            
            if not scraped or not hasattr(scraped, 'markdown'):
                print("‚ùå Falha no scraping")
                return False
            
            content = scraped.markdown
            # Conte√∫do extra√≠do em markdown
            
            # Chunking do conte√∫do
            chunks = self.text_splitter.split_text(content)
            # Divide em peda√ßos process√°veis
            
            # Cria documentos RAG
            documents = []
            for i, chunk in enumerate(chunks):
                doc = RAGDocument(
                    id=f"{url}#{i}",
                    # ID √∫nico: URL + √≠ndice do chunk
                    content=chunk,
                    metadata={
                        "source_url": url,
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "scraped_at": str(asyncio.get_event_loop().time())
                        # Timestamp para tracking
                    }
                )
                documents.append(doc)
            
            # Adiciona ao Pinecone
            success = await self.pinecone_service.add_documents(documents)
            
            if success:
                print(f"‚úÖ {len(documents)} chunks adicionados da URL: {url}")
            
            return success
            
        except Exception as e:
            print(f"‚ùå Erro ao adicionar conhecimento: {e}")
            return False
    
    async def add_knowledge_from_text(self, text: str, source_id: str) -> bool:
        """Adiciona conhecimento a partir de texto"""
        # M√©todo alternativo: texto direto sem scraping
        # source_id: identificador da fonte
        
        try:
            # Chunking do texto
            chunks = self.text_splitter.split_text(text)
            
            # Cria documentos
            documents = []
            for i, chunk in enumerate(chunks):
                doc = RAGDocument(
                    id=f"{source_id}#{i}",
                    content=chunk,
                    metadata={
                        "source_id": source_id,
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "added_at": str(asyncio.get_event_loop().time())
                    }
                )
                documents.append(doc)
            
            # Adiciona ao Pinecone
            success = await self.pinecone_service.add_documents(documents)
            
            if success:
                print(f"‚úÖ {len(documents)} chunks adicionados do texto: {source_id}")
            
            return success
            
        except Exception as e:
            print(f"‚ùå Erro ao adicionar texto: {e}")
            return False
    
    async def query(self, user_query: str, top_k: int = 4, threshold: float = 0.1) -> RAGResponse:
        """Processa query usando RAG"""
        # M√©todo principal: implementa pipeline RAG completo
        # Query ‚Üí Retrieval ‚Üí Augmentation ‚Üí Generation
        
        try:
            print(f"üîç Processando query: {user_query}")
            
            # 1. RETRIEVAL: Busca documentos relevantes
            relevant_docs = await self.pinecone_service.search(
                query=user_query,
                top_k=top_k,
                threshold=threshold
            )
            
            if not relevant_docs:
                # Fallback quando n√£o h√° documentos relevantes
                return RAGResponse(
                    answer="N√£o encontrei informa√ß√µes relevantes na base de conhecimento para responder sua pergunta. Tente reformular ou adicionar mais contexto.",
                    sources=[],
                    query=user_query,
                    confidence=0.0
                )
            
            # 2. AUGMENTATION: Prepara contexto
            context = self._build_context(relevant_docs)
            # Combina documentos em contexto unificado
            
            # 3. GENERATION: Gera resposta com LLM
            messages = [
                SystemMessage(content=self.system_prompt),
                HumanMessage(content=f"""Contexto dos documentos:
                {context}
                Pergunta do usu√°rio: {user_query}
                Por favor, responda baseando-se apenas nas informa√ß√µes fornecidas no contexto.""")
            ]
            
            response = self.llm.invoke(messages)
            # LLM gera resposta com contexto aumentado
            
            # Calcula confian√ßa baseada nos scores
            confidence = self._calculate_confidence(relevant_docs)
            
            return RAGResponse(
                answer=response.content,
                sources=relevant_docs,
                query=user_query,
                confidence=confidence
            )
            
        except Exception as e:
            print(f"‚ùå Erro no RAG query: {e}")
            return RAGResponse(
                answer=f"Erro ao processar consulta: {str(e)}",
                sources=[],
                query=user_query,
                confidence=0.0
            )
    
    def _build_context(self, documents: List[RAGDocument]) -> str:
        """Constr√≥i contexto a partir dos documentos"""
        # M√©todo privado: combina documentos em texto unificado
        # Otimiza formato para consumo do LLM
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            # Enumera documentos para refer√™ncia
            
            context_part = f"""--- Documento {i} (Score: {doc.score:.3f}) ---
            Fonte: {doc.metadata.get('source_url', doc.metadata.get('source_id', 'Desconhecido'))}
            Conte√∫do: {doc.content}
            """
            context_parts.append(context_part)
        
        return "\n\n".join(context_parts)
        # Junta com separadores claros
    
    def _calculate_confidence(self, documents: List[RAGDocument]) -> float:
        """Calcula confian√ßa baseada nos scores dos documentos"""
        # Heur√≠stica: confian√ßa baseada na qualidade dos matches
        
        if not documents:
            return 0.0
        
        # M√©dia ponderada dos scores
        scores = [doc.score for doc in documents if doc.score]
        if not scores:
            return 0.5  # Default quando scores n√£o dispon√≠veis
        
        # Peso maior para o melhor resultado
        weighted_avg = sum(score * (len(scores) - i) for i, score in enumerate(scores)) / sum(range(1, len(scores) + 1))
        
        return min(weighted_avg, 1.0)
        # Garante que n√£o excede 1.0
    
    async def get_knowledge_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas da base de conhecimento"""
        # M√©todo de diagn√≥stico
        
        pinecone_stats = await self.pinecone_service.get_stats()
        # Stats do Pinecone
        
        return {
            "rag_agent": {
                "status": "active",
                "model": "gpt-4.1-mini",
                "embedding_model": "text-embedding-3-small"
            },
            "knowledge_base": pinecone_stats,
            "capabilities": [
                "Semantic search",
                "Document chunking", 
                "Source citation",
                "Confidence scoring"
            ]
        }
    
    async def suggest_knowledge_sources(self, domain: str) -> List[str]:
        """Sugere fontes de conhecimento para um dom√≠nio"""
        # Feature: sugest√£o de URLs relevantes para indexar
        # domain: √°rea de conhecimento (ex: "machine learning")
        
        if not self.firecrawl:
            return []
        
        try:
            # Busca fontes relevantes
            search_query = f"{domain} documentation tutorial guide"
            results = self.firecrawl.search(search_query, limit=4)
            
            if hasattr(results, 'data') and results.data:
                return [result.get('url', '') for result in results.data if result.get('url')]
            
            return []
            
        except Exception as e:
            print(f"‚ùå Erro ao sugerir fontes: {e}")
            return []